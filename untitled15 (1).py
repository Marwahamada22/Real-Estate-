# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oNYovRoAWTA5Jl0cc3J4NOrngiDhiKwL
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, recall_score, precision_score, classification_report
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

df_s=pd.read_csv('sample_submission.csv')
df_s

df_s.info()

df_s.describe()

df_s.isnull().sum()

from IPython.display import HTML

HTML('<span style="color: red;font-size: 40px;">END PREPROCESS OF FIRST FILE (SAMPLE_SUBMISSION)</span>')

df_tr=pd.read_csv('train.csv')
df_tr

df_tr.info()

df_tr.describe()

df_tr.isnull().sum()

null_columns = df_tr.columns[df_tr.isnull().any()]

# Convert to list if needed
null_columns_list = null_columns.tolist()

print(null_columns_list)

for col in df_tr.select_dtypes(include=['object']).columns:
    df_tr[col] = df_tr[col].astype('category')

# Check the DataFrame dtypes
print(df_tr.dtypes)

category_columns = df_tr.select_dtypes(include=['category']).columns

# Fill NaN values with the mode of each categorical column
for col in category_columns:
    # Get mode
    mode_value = df_tr[col].mode()[0]
    # Fill NaN values
    df_tr[col].fillna(mode_value, inplace=True)

# Check the result
print(df_tr)

# Fill NaN values in numeric columns with mean
num_columns = df_tr.select_dtypes(include=['float', 'int']).columns

for col in num_columns:
    mean_value = df_tr[col].mean()  # No indexing needed
    df_tr[col].fillna(mean_value, inplace=True)

# Check the result
print(df_tr)

df_tr.info()

#import pandas as pd

#categorical_columns = df_tr.select_dtypes(include=['category']).columns


#df_tr_one_hot_encoded = pd.get_dummies(df_tr, columns=categorical_columns)
#print(df_tr_one_hot_encoded)

import pandas as pd
from sklearn.preprocessing import LabelEncoder

categorical_columns = df_tr.select_dtypes(include=['category']).columns

label_encoder = LabelEncoder()

for col in categorical_columns:
    df_tr[col] = label_encoder.fit_transform(df_tr[col])

df_tr

df_tr.to_csv('marwa.csv')

df_tr.info()

from IPython.display import HTML

HTML('<span style="color: red;font-size: 40px;">END PREPROCESS OF second FILE (TRAIN)</span>')

from IPython.display import HTML

HTML('<span style="color: yellow;font-size: 40px;">Data Visualization 1 (train)</span>')

from IPython.display import HTML

HTML('<span style="color: yellow;font-size: 40px;">Data Visualization 1 (train)</span>')

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
sns.histplot(df_tr['SalePrice'], kde=True)
plt.title('Distribution of SalePrice')
plt.xlabel('SalePrice')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(x='LotArea', y='SalePrice', data=df_tr)
plt.title('LotArea vs SalePrice')
plt.xlabel('LotArea')
plt.ylabel('SalePrice')
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='MSZoning', y='SalePrice', data=df_tr)
plt.title('SalePrice by MSZoning')
plt.xlabel('MSZoning')
plt.ylabel('SalePrice')
plt.show()

sns.pairplot(df_tr[['LotArea', 'GrLivArea', 'TotalBsmtSF', 'SalePrice']])
plt.show()

plt.figure(figsize=(150,180))
sns.heatmap(df_tr.corr(), annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()











from IPython.display import HTML

HTML('<span style="color: yellow;font-size: 40px;">END Data Visualization 1 (train)</span>')

df_t=pd.read_csv('test.csv')
df_t

df_t.describe()

df_t.info()

for col in df_t.select_dtypes(include=['object']).columns:
    df_t[col] = df_t[col].astype('category')

# Check the DataFrame dtypes
print(df_t.dtypes)

category_columns = df_t.select_dtypes(include=['category']).columns

# Fill NaN values with the mode of each categorical column
for col in category_columns:
    # Get mode
    mode_value = df_t[col].mode()[0]
    # Fill NaN values
    df_t[col].fillna(mode_value, inplace=True)

# Check the result
print(df_t)

# Fill NaN values in numeric columns with mean
num_columns = df_t.select_dtypes(include=['float', 'int']).columns

for col in num_columns:
    mean_value = df_t[col].mean()  # No indexing needed
    df_t[col].fillna(mean_value, inplace=True)

# Check the result
print(df_t)

df_t.isnull().sum()

df_t.info()



import pandas as pd
from sklearn.preprocessing import LabelEncoder

categorical_columns = df_t.select_dtypes(include=['category']).columns

label_encoder = LabelEncoder()

for col in categorical_columns:
    df_t[col] = label_encoder.fit_transform(df_t[col])

df_t

df_t.info()

from IPython.display import HTML

HTML('<span style="color: red;font-size: 40px;">END PREPROCESS OF Third FILE (Test)</span>')

df_t.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8);







df_tr.reset_index(drop=True, inplace=True)
df_t.reset_index(drop=True, inplace=True)

# Concatenate the DataFrames, ensuring only common columns are included
combined_df = pd.concat([df_tr, df_t], join='inner', ignore_index=True)

# Check for null values in the combined DataFrame
print("Null values in combined DataFrame:")
print(combined_df.isnull().sum())

# Perform analysis on the combined DataFrame

# 1. Summary statistics
print("Summary Statistics:")
print(combined_df.describe())

# 2. Check for missing values again
print("\nMissing Values:")
print(combined_df.isnull().sum())

combined_df.to_csv('data.csv')

data=pd.read_csv('Data.csv')
data

# Handle missing values
data = data.dropna()

# Encode categorical variables
data = pd.get_dummies(data)

# Define feature variables (X) and target variable (y)
X = data.drop('SalePrice', axis=1)
y = data['SalePrice']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Initialize the model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print(f'Random Forest MSE: {mse_rf}')
print(f'Random Forest R2: {r2_rf}')

from sklearn.tree import DecisionTreeRegressor

# Initialize the model
cart_model = DecisionTreeRegressor(random_state=42)

# Train the model
cart_model.fit(X_train, y_train)

# Make predictions
y_pred_cart = cart_model.predict(X_test)

# Evaluate the model
mse_cart = mean_squared_error(y_test, y_pred_cart)
r2_cart = r2_score(y_test, y_pred_cart)

print(f'CART Model MSE: {mse_cart}')
print(f'CART Model R2: {r2_cart}')





import joblib

# Save the RandomForest model
joblib.dump(rf_model, 'random_forest_model.pkl')

pip install gradio

import gradio as gr

import gradio as gr
import joblib
import pandas as pd

# Load the model
model = joblib.load('random_forest_model.pkl')

# Define a function for making predictions
def predict_house_price(
    OverallQual, GrLivArea, GarageCars, TotalBsmtSF, FullBath, YearBuilt, YearRemodAdd, Neighborhood, MSZoning, LotArea
):
    input_data = pd.DataFrame({
        'OverallQual': [OverallQual], 'GrLivArea': [GrLivArea], 'GarageCars': [GarageCars], 'TotalBsmtSF': [TotalBsmtSF],
        'FullBath': [FullBath], 'YearBuilt': [YearBuilt], 'YearRemodAdd': [YearRemodAdd], 'Neighborhood': [Neighborhood],
        'MSZoning': [MSZoning], 'LotArea': [LotArea]
    })

    prediction = model.predict(input_data)[0]
    return prediction

# Define Gradio interface
iface = gr.Interface(
    fn=predict_house_price,
    inputs=[
        gr.Number(label="Overall Quality"), gr.Number(label="Above Ground Living Area (sq ft)"),
        gr.Number(label="Garage Cars"), gr.Number(label="Total Basement Area (sq ft)"),
        gr.Number(label="Full Baths"), gr.Number(label="Year Built"),
        gr.Number(label="Year Remodeled"), gr.Textbox(label="Neighborhood"),
        gr.Textbox(label="Zoning Class"), gr.Number(label="Lot Area (sq ft)")
    ],
    outputs=gr.Number(label="Predicted Sale Price"),
    title="House Price Predictor",
    description="Enter the following features to predict the house price."
)

# Launch the interface
iface.launch()

import gradio as gr
import joblib
import pandas as pd

# Load the model
model = joblib.load('random_forest_model.pkl')

# Define a function for making predictions
def predict_house_price(
    OverallQual, GrLivArea, GarageCars, TotalBsmtSF, FullBath, YearBuilt, YearRemodAdd, Neighborhood, MSZoning, LotArea
):
    input_data = pd.DataFrame({
        'OverallQual': [OverallQual],  'GarageCars': [GarageCars],
        'FullBath': [FullBath], 'YearBuilt': [YearBuilt], 'Neighborhood': [Neighborhood],

    })

    prediction = model.predict(input_data)[0]
    return prediction

# Define Gradio interface
iface = gr.Interface(
    fn=predict_house_price,
    inputs=[
        gr.Number(label="Overall Quality"),
        gr.Number(label="Garage Cars"),
        gr.Number(label="Full Baths"), gr.Number(label="Year Built"),
         gr.Number(label="Neighborhood"),

    ],
    outputs=gr.Number(label="Predicted Sale Price"),
    title="House Price Predictor",
    description="Enter the following features to predict the house price."
)

# Launch the interface
iface.launch()

import gradio as gr
import joblib
import pandas as pd

# Load the model
model = joblib.load('random_forest_model.pkl')

# Define a function for making predictions
def predict_house_price(
    OverallQual,  GarageCars,  FullBath, YearBuilt, YearRemodAdd, Neighborhood
):
    # Create a DataFrame with the input data
    input_data = pd.DataFrame({
        'OverallQual': [OverallQual],

        'GarageCars': [GarageCars],
        'FullBath': [FullBath],
        'YearBuilt': [YearBuilt],
        'YearRemodAdd': [YearRemodAdd],
        'Neighborhood': [Neighborhood],

    })

    # Make a prediction
    prediction = model.predict(input_data)[0]
    return prediction

# Define Gradio interface
iface = gr.Interface(
    fn=predict_house_price,
    inputs=[
        gr.Number(label="Overall Quality"),
        gr.Number(label="Garage Cars"),
        gr.Number(label="Full Baths"),
        gr.Number(label="Year Built"),
        gr.Number(label="Year Remodeled"),
        gr.Textbox(label="Neighborhood"),

    ],
    outputs=gr.Number(label="Predicted Sale Price"),
    title="House Price Predictor",
    description="Enter the following features to predict the house price."
)

# Launch the interface
iface.launch()

